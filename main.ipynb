{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse    #python用于解析命令行参数和选项的标准模块，用于代替已经过时的optparse模块\n",
    "import os          #导入操作系统接口模块\n",
    "import re          #正则表达式\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable #专门为了BP算法设计的，只对输出值是标量的有用（variable是tensor的外包装，variable类型变量的data属性存储着tensor数据，grad属性存储关于该变量的导数，creator是代表该变量的创造者）\n",
    "import torch.nn as nn               #主要包含了用来搭建各个层的模块（Modules），比如全连接、二维卷积、池化等；\n",
    "import torch.optim as optim         #  包含了用来更新参数的优化算法，比如SGD、AdaGrad、RMSProp、 Adam等\n",
    "import dataset                      #分布式的数据集合。   \n",
    "from cnn_finetune import make_model #尝试对模型进行微调，以进一步提升模型性能\n",
    "from tqdm import tqdm               #Tqdm 是一个快速，可扩展的Python进度条，可以在 Python 长循环中添加一个进度提示信息，用户只需要封装任意的迭代器 tqdm(iterator)。\n",
    "from PIL import Image               #调用库，包含图像类\n",
    "from collections import defaultdict #defaultdict的作用是在于，当字典里的key不存在但被查找时，返回的不是keyError而是一个默认值\n",
    "import numpy as np                  #NumPy系统是Python的一种开源的数值计算扩展\n",
    "import matplotlib.pyplot as plt     #一个绘图库，是Python中最常用的可视化工具之一，可以非常方便地创建2D图表和一些基本的3D图表\n",
    "import warnings     \n",
    "from torch.serialization import SourceChangeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=SourceChangeWarning)   #抑制显示的特定警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数解释  ：argparse是是Python标准库中推荐使用的编写命令行程序的工具，是一个用来解析命令行程序的参数的模块。\n",
    "# type:输入值的类型\n",
    "# help:该参数的描述，用在--help中\n",
    "# metavar:在--help中的参数名称\n",
    "# default:默认值\n",
    "\n",
    "# if 1:\n",
    "def test_triplet():     \n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Face recognition using triplet loss.')\n",
    "    parser.add_argument('--train-set', type=str, default='train_set', metavar='T',   #字符串类:训练集的路径      metavar:元变量：它为帮助消息中的可选参数提供了不同的名称。在add_argument（）中为metavar关键字参数提供一个值\n",
    "                        help='path of train set.')\n",
    "    parser.add_argument('--batch-size', type=int, default=32, metavar='N',           #整数型：batch size for training \n",
    "                        help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',        #整数型：：batch size for testing\n",
    "                        help='input batch size for testing (default: 64)')\n",
    "    parser.add_argument('--epochs', type=int, default=5, metavar='N',                 #整数型：训练的epochs\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "    parser.add_argument('--lr', type=float, default=0.005, metavar='LR',              #浮点型：学习率LR\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',           #浮点型：momentum 动量，作用是尽量保持当前梯度的变化方向\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,              #没有cuda的情况\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',                    #随机数种子：通过随机种子，通过一些复杂的数学算法，你可以得到一组有规律的随机数，而随机种子就是这个随机数的初始值。随机种子相同，得到的随机数一定也相同。\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')  #打印训练状态时等待的的batch数\n",
    "    parser.add_argument('--model-name', type=str, default='resnet18', metavar='M',\n",
    "                        help='model name (default: resnet50)')                     \n",
    "    parser.add_argument('--dropout-p', type=float, default=0.2, metavar='D',             #浮点型：dropout(在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络)\n",
    "                        help='Dropout probability (default: 0.2)')\n",
    "    parser.add_argument('--check-path', type=str,                                       #字符串：模型节点路径\n",
    "                        default='checkpoints', metavar='C', help='Checkpoint path')\n",
    "    parser.add_argument('--is-resume', type=bool, default=True,                           #布尔型：是否从最近一次节点恢复\n",
    "                        metavar='R', help='whether resume from latest checkpoint.')\n",
    "\n",
    "    # ----------------------参数\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    # ----------------------模型\n",
    "    if args.is_resume:  # 从checkpoint恢复模型\n",
    "        checkpoints = os.listdir(args.check_path)\n",
    "        checkpoints.sort(key=lambda x: int(re.match(r'epoch_(\\d+)\\.pth', x).group(1)),\n",
    "                         reverse=True)\n",
    "        model = torch.load(os.path.join(\n",
    "            args.check_path + os.path.sep + checkpoints[0]),map_location=torch.device('cpu'))\n",
    "        LATEST_MODEL_ID = int(\n",
    "            re.match(r'epoch_(\\d+)\\.pth', checkpoints[0]).group(1))\n",
    "        print('[resume from model, model id: %d]' % LATEST_MODEL_ID)\n",
    "    else:  # 从训练好的模型加载\n",
    "        model = make_model(args.model_name,\n",
    "                           pretrained=True,\n",
    "                           num_classes=62,\n",
    "                           dropout_p=args.dropout_p)\n",
    "    \n",
    "    \n",
    "    if args.is_resume:  # 从训练好的模型加载\n",
    "        model = make_model(args.model_name,\n",
    "                           pretrained=True,\n",
    "                           num_classes=62,\n",
    "                           dropout_p=args.dropout_p)\n",
    "\n",
    "    # else:  #从checkpoint恢复模型\n",
    "    #     checkpoints = os.listdir(args.check_path)\n",
    "    #     checkpoints.sort(key=lambda x: int(re.match(r'epoch_(\\d+)\\.pth', x).group(1)),\n",
    "    #                      reverse=True)\n",
    "    #     model = torch.load(os.path.join(\n",
    "    #         args.check_path + os.path.sep + checkpoints[0]), map_location='cpu')    #这里改成了使用CPU\n",
    "           \n",
    "    #     LATEST_MODEL_ID = int(\n",
    "    #         re.match(r'epoch_(\\d+)\\.pth', checkpoints[0]).group(1))\n",
    "    #     print('[resume from model, model id: %d]' % LATEST_MODEL_ID)    #获得恢复的模型\n",
    "    # # print('model:\\n', model)\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "\n",
    "    # ----------------------对图片数据处理: 转换成Tensor并中心归一化\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=np.array([0.485, 0.456, 0.406]),\n",
    "            std=np.array([0.229, 0.224, 0.225])),\n",
    "    ])\n",
    "\n",
    "    # ----------------------加载训练数据集\n",
    "    # train_set = dataset.Triplet(args.train_set,\n",
    "    #                             num_cls=62,  # 62种类\n",
    "    #                             num_tripets=8000,\n",
    "    #                             limit=20,  # 20\n",
    "    #                             transforms=transform,\n",
    "    #                             train=True,\n",
    "    #                             test=False)\n",
    "    train_set = dataset.Hard_Triplet(args.train_set,\n",
    "                                     'checkpoints/epoch_35.pth') # 先人为指定...\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               shuffle=True,          #用于打乱数据集，每次都会以不同的顺序返回\n",
    "                                               num_workers=2)\n",
    "\n",
    "    # ----------------------加载测试数据集\n",
    "    test_set = dataset.FACE_LFW(\n",
    "        args.train_set, transforms=transform, NUM_PER_CLS=20)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                              args.test_batch_size,\n",
    "                                              shuffle=False,           \n",
    "                                              num_workers=2)\n",
    "\n",
    "    # ----------------------可视化训练数据\n",
    "    def imshow(img, title=None):\n",
    "        \"\"\"Imshow for Tensor.\"\"\"\n",
    "        # (channels,imagesize,imagesize) -> (imagesize,imagesize,channels)\n",
    "        img = img.numpy().transpose((1, 2, 0))  # 将Tensor中的数据格式转换用于plt显示的格式\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "\n",
    "        plt.imshow(img)\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    # for i in range(4):  # 总共迭代4个batch的数据\n",
    "    #     inputs, classes = next(iter(train_loader))  # 迭代一个batch的训练数据集\n",
    "    #     out = torchvision.utils.make_grid(\n",
    "    #         inputs[2])  # 每个batch有4个数据，每个数据包含3张图片的数据\n",
    "    #     imshow(out)\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # ----------------------训练&测试\n",
    "    # 损失函数\n",
    "    criterion = nn.CrossEntropyLoss()           #计算交叉熵损失(是nn.logSoftmax()和nn.NLLLoss()的整合,可以直接使用它来替换网络中的这两个操作。)\n",
    "    triplet_loss = nn.TripletMarginLoss(margin=1.2, p=2)  # 三元损失：学习目标是让Positive和Anchor之间的距离 D ( a , p ) D(a,p)D(a,p) 尽可能的小，Negative和Anchor之间的距离 D ( a , n ) D(a,n)D(a,n) 尽可能的大：   优化margin\n",
    "\n",
    "# 函数原型:CLASS torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "# margin (float) – 默认为1\n",
    "# p (int) – norm degree，默认为2\n",
    "# swap (bool) – The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. 默认为False\n",
    "# size_average (bool) – Deprecated\n",
    "# reduce (bool) – Deprecated\n",
    "# reduction (string) – 指定返回各损失值(none)，批损失均值(mean)，批损失和(sum)，默认返回批损失均值(mean)\n",
    "\n",
    "\n",
    "\n",
    "    # 优化器\n",
    "    optimizer = optim.SGD(model.parameters(),            #SGD就是随机梯度下降\n",
    "                          lr=args.lr,                       #学习率较小时，收敛到极值的速度较慢。学习率较大时，容易在搜索过程中发生震荡。\n",
    "                          momentum=args.momentum,            #动量加速\n",
    "                          weight_decay=1e-5)  #   为了有效限制模型中的自由参数数量以避免过度拟合，可以调整成本函数。权值衰减: 加入L2正则?\n",
    "\n",
    "    def train(epoch):\n",
    "        model.train()  # 网络在train模式\n",
    "        total_loss = 0.0\n",
    "        total_size = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):  # 一个batch    enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中,  target是真实标签\n",
    "            # if args.cuda:\n",
    "            #     data[0], target[0] = data[0].cuda(), target[0].cuda()    #没有cuda\n",
    "            #     data[1], target[1] = data[1].cuda(), target[1].cuda()\n",
    "            #     data[2], target[2] = data[2].cuda(), target[2].cuda()\n",
    "                \n",
    "            data[0], target[0] = data[0].cpu(), target[0].cpu()\n",
    "            data[1], target[1] = data[1].cpu(), target[1].cpu()\n",
    "            data[2], target[2] = data[2].cpu(), target[2].cpu()\n",
    "\n",
    "            data[0], target[0] = Variable(data[0]), Variable(target[0])\n",
    "            data[1], target[1] = Variable(data[1]), Variable(target[1])\n",
    "            data[2], target[2] = Variable(data[2]), Variable(target[2])\n",
    "\n",
    "            optimizer.zero_grad()   #参数梯度置零:根据pytorch中的backward()函数的计算，当网络参量进行反馈时，梯度是被积累的而不是被替换掉；但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积，因此这里就需要每个batch设置一遍zero_grad 了。\n",
    "\n",
    "\n",
    "            # 计算特征向量\n",
    "            anchor = model.forward(data[0])     #module(data)  等价于 module.forward(data),因为有python calss 中的__call__和__init__方法.  \n",
    "            positive = model.forward(data[1])\n",
    "            negative = model.forward(data[2])\n",
    "\n",
    "            # 计算分类loss\n",
    "            loss_cls_0 = criterion(anchor, target[0].long())             #分别计算三类的交叉熵损失\n",
    "            loss_cls_1 = criterion(positive, target[1].long())\n",
    "            loss_cls_2 = criterion(negative, target[2].long())\n",
    "            loss_cls = loss_cls_0 + loss_cls_1 + loss_cls_2         #分类损失\n",
    "\n",
    "            # 计算三元组loss\n",
    "            loss_tri = triplet_loss.forward(anchor, positive, negative)         #计算三元组损失(anchor, positive, negative)\n",
    "\n",
    "            # 分类loss + triplet loss: 权重如何分配?\n",
    "            loss = loss_tri + loss_cls        # 总体损失定义为分类损失和三元组损失之和\n",
    "\n",
    "            # 统计loss\n",
    "            total_loss += loss.data.cpu()[0]       #累加起来\n",
    "            total_size += data[0].size(0)\n",
    "\n",
    "            loss.backward()     #反向传播计算得到每个参数的梯度值（loss.backward()）\n",
    "            optimizer.step()    #通过梯度下降执行一步参数更新（optimizer.step()）\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:           #这一步是说batch恰好被等待个数整除？\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)], Average loss: {:.4f}'.format(             #Average loss = total_loss / total_size          \n",
    "                    epoch, batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader), total_loss / total_size))\n",
    "\n",
    "        if args.is_resume:                                     #开始回复之前的节点模型\n",
    "            model_path = args.check_path + \\\n",
    "                os.path.sep + 'epoch_{}.pth'.format(epoch)\n",
    "            if os.path.exists(model_path):\n",
    "                # 如果已经存在, 重命名模型\n",
    "                print('the model already exists, rename the model and save.')\n",
    "                ID = (epoch + 1) + LATEST_MODEL_ID\n",
    "                print('new_id: ', ID)\n",
    "                model_path = args.check_path + os.path.sep + \\\n",
    "                    'epoch_{}.pth'.format(ID)\n",
    "            torch.save(model, model_path)\n",
    "        else:                                     #和上面的有区别吗？\n",
    "            if epoch % 10 == 0:\n",
    "                model_path = args.check_path + \\\n",
    "                    os.path.sep + 'epoch_{}.pth'.format(epoch)\n",
    "                if os.path.exists(model_path):\n",
    "                    # 如果已经存在, 重命名模型\n",
    "                    print('the model already exists, rename the model and save.')\n",
    "                    ID = epoch + LATEST_MODEL_ID\n",
    "                    model_path = args.check_path + os.path.sep + \\\n",
    "                        'epoch_{}.pth'.format(ID)\n",
    "                torch.save(model, model_path)\n",
    "        print('model {} saved.'.format(model_path))              \n",
    "\n",
    "\n",
    "#  定义测试模式\n",
    "# 在train模式下，dropout网络层会按照设定的参数p设置保留激活单元的概率（保留概率=p）;batchnorm层会继续计算数据的mean和var等参数并更新。\n",
    "\n",
    "# 在val模式下，dropout层会让所有的激活单元都通过，而batchnorm层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值。\n",
    "\n",
    "# 该模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反传（backprobagation）\n",
    "\n",
    "# 而with torch.no_grad（）则主要是用于停止autograd模块的工作，以起到加速和节省显存的作用，具体行为就是停止gradient计算，从而节省了GPU算力和显存，但是并不会影响dropout和batchnorm层的行为。\n",
    "    \n",
    "    def test():  \n",
    "\n",
    "        model.eval()  # 网络在测试模式\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            # if args.cuda:                                       #没有cuda\n",
    "            #     data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            data, target = data.cpu(), target.cpu()\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "\n",
    "            output = model.forward(data)  # 预测\n",
    "\n",
    "            test_loss += criterion(output, target).data.cpu()[0]   #交叉熵损失的和\n",
    "            pred = output.data.max(1, keepdim=True)[1]           # keepdim（bool）– 保持输出的维度 。 当keepdim=False时，输出比输入少一个维度（就是指定的dim求范数的维度）。\n",
    "                                                                 # 而keepdim=True时，输出与输入维度相同，仅仅是输出在求范数的维度上元素个数变为1。这也是为什么有时我们把参数中的dim称为缩减的维度，因为norm运算之后，此维度或者消失或者元素个数变为1。\n",
    "\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()         #实际标签和预测结果相同的个数之和就是正确的数量\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)         #测试集的损失=交叉熵损失和/数据长度\n",
    "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(        #分别打印出来test_loss和correct   \n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / float(len(test_loader.dataset))))            #计算\n",
    "\n",
    "    for epoch in range(args.epochs):           #循环\n",
    "        train(epoch)\n",
    "        test()\n",
    "        validate(args.check_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------验证数据集\n",
    "def validate(check_path):\n",
    "    if not os.path.exists(check_path):\n",
    "        print('Error: invalid checkpoints path.')\n",
    "        return\n",
    "    print('Checkpoint path: ', check_path)\n",
    "\n",
    "    # 加载网络\n",
    "    checkpoints = os.listdir(check_path)                 #先从路径中加载之前回复的网络结构\n",
    "    checkpoints.sort(key=lambda x: int(re.match(r'epoch_(\\d+)\\.pth', x).group(1)),\n",
    "                     reverse=True)\n",
    "    model_path = os.path.join(check_path + os.path.sep + checkpoints[0])\n",
    "    print('model: {}'.format(model_path))\n",
    "\n",
    "    model = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "    model.eval()  # 网络在求值模式\n",
    "\n",
    "    # 数据处理方式\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=np.array([0.485, 0.456, 0.406]),\n",
    "            std=np.array([0.229, 0.224, 0.225])),\n",
    "    ])\n",
    "\n",
    "    # 加载数据\n",
    "    valid_set = dataset.FACE_LFW('validate_set',\n",
    "                                 transforms=transform,\n",
    "                                 NUM_PER_CLS=10)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set,\n",
    "                                               4,             #batch-size\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=2)\n",
    "    criterion = nn.CrossEntropyLoss()     #交叉熵损失\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    for data, target in tqdm(valid_loader):\n",
    "        # if is_cuda:                                     #没有cuda直接不判断了\n",
    "        #     data, target = data.cuda(), target.cuda()\n",
    "        # else:\n",
    "        #     data, target = data.cpu(), target.cpu()\n",
    "        \n",
    "        data, target = data.cpu(), target.cpu()\n",
    "            \n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "\n",
    "        output = model.forward(data)  # 预测\n",
    "\n",
    "        valid_loss += criterion(output, target).data.cpu()[0]\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    valid_loss /= float(len(valid_loader.dataset))\n",
    "    print('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        valid_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / float(len(valid_loader.dataset))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resume from model, model id: 35]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2000/2998192347.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtest_triplet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# validate('checkpoints')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# validate('resnet_checkpoints')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2000/3582856085.py\u001b[0m in \u001b[0;36mtest_triplet\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m#                             test=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     train_set = dataset.Hard_Triplet(args.train_set,\n\u001b[1;32m--> 101\u001b[1;33m                                      'checkpoints/epoch_35.pth') # 先人为指定...\n\u001b[0m\u001b[0;32m    102\u001b[0m     train_loader = torch.utils.data.DataLoader(train_set,\n\u001b[0;32m    103\u001b[0m                                                \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PyCharm\\Projects\\facenet\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, model_path, transform)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mhard\u001b[0m \u001b[0mtriplets初始化\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m数据预处理方式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         '''\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriplets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_hard_triplets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m62\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m# 数据预处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PyCharm\\Projects\\facenet\\utils.py\u001b[0m in \u001b[0;36mget_all_hard_triplets\u001b[1;34m(dir, model_path, num_classes, num_per_cls)\u001b[0m\n\u001b[0;32m    825\u001b[0m     '''\n\u001b[0;32m    826\u001b[0m     \u001b[1;31m# 加载特征提取CNN模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[1;31m# 将全连接层分类器转换成为恒等映射, 用来提取深度特征\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    606\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    741\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[0;32m    136\u001b[0m                            \u001b[1;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                            \u001b[1;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_triplet()\n",
    "    \n",
    "    # validate('checkpoints')\n",
    "    # validate('resnet_checkpoints')\n",
    "    # validate_statics('checkpoints')\n",
    "\n",
    "\n",
    "# https://github.com/adambielski/siamese-triplet (pytorch triplet loss)\n",
    "# https://www.ddvip.com/weixin/20171218A0236200.html (pytorch显存占用分析)\n",
    "# https://blog.csdn.net/qq_14845119/article/details/76083042 (车型分类博客)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e351b8f43a8051a413352beddec2a7234c5048dc672983c8d1a2c12b0911bb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
