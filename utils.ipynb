{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dir, num_cls, num_per_cls, val_num):\n",
    "    '''\n",
    "    split data set into train and \n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: dir not exists')\n",
    "        return\n",
    "\n",
    "    data_dir = dir + os.path.sep + 'data'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    # os.chdir(data_dir) # 切换目录\n",
    "    train_dir = data_dir + os.path.sep + 'train'\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    val_dir = data_dir + os.path.sep + 'val'\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.makedirs(val_dir)\n",
    "\n",
    "    # 在train目录中创建每个类别的目录\n",
    "    train_sub_dirs, val_sub_dirs = create_sub_dirs_ID(\n",
    "        num_cls, train_dir, val_dir)\n",
    "\n",
    "    # 删除目录, 只保留文件\n",
    "    i = 0\n",
    "    f_names = os.listdir(dir)\n",
    "    for item in f_names:\n",
    "        if os.path.isdir(os.path.join(dir, item)):\n",
    "            f_names.remove(item)\n",
    "\n",
    "    print(len(f_names))\n",
    "    # f_names = ['s165.bmp', 's17.bmp', 's18.bmp']\n",
    "    f_names.sort(key=lambda x: int(re.match(r's(\\d+)\\.bmp', x).group(1)))\n",
    "    for f in f_names:\n",
    "        f_name = os.path.join(dir, f)\n",
    "        if os.path.isfile(f_name):\n",
    "            # print(f_name)\n",
    "            if os.path.splitext(f_name)[1] == '.bmp':\n",
    "                img = cv2.imread(f_name, cv2.IMREAD_UNCHANGED)\n",
    "                file_cvt = os.path.splitext(f_name)[0] + '.png'\n",
    "                cv2.imwrite(file_cvt, img)  # 写入png文件\n",
    "                for j in range(num_cls):\n",
    "                    if int(i / num_per_cls) == j:\n",
    "                        if (i % num_per_cls) < (num_per_cls - val_num):\n",
    "                            f_move = train_sub_dirs[j] + os.path.sep + \\\n",
    "                                os.path.split(os.path.realpath(file_cvt))[1]\n",
    "                            if not os.path.exists(f_move):\n",
    "                                shutil.move(file_cvt, train_sub_dirs[j])\n",
    "                                break\n",
    "                        else:\n",
    "                            f_move = val_sub_dirs[j] + os.path.sep + \\\n",
    "                                os.path.split(os.path.realpath(file_cvt))[1]\n",
    "                            if not os.path.exists(f_move):\n",
    "                                shutil.move(file_cvt, val_sub_dirs[j])\n",
    "                                break\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_dirs_ID(num_cls, train_dir, val_dir):\n",
    "    train_sub_dirs = []\n",
    "    val_sub_dirs = []\n",
    "    for i in range(num_cls):\n",
    "        train_sub_dir = train_dir + os.path.sep + str(i)\n",
    "        if not os.path.exists(train_sub_dir):\n",
    "            os.makedirs(train_sub_dir)\n",
    "        train_sub_dirs.append(train_sub_dir)\n",
    "        val_sub_dir = val_dir + os.path.sep + str(i)\n",
    "        if not os.path.exists(val_sub_dir):\n",
    "            os.makedirs(val_sub_dir)\n",
    "        val_sub_dirs.append(val_sub_dir)\n",
    "    return train_sub_dirs, val_sub_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_dirs_Name(cls_names, train_dir, val_dir):\n",
    "    train_sub_dirs = []\n",
    "    val_sub_dirs = []\n",
    "    for i in range(len(cls_names)):\n",
    "        train_sub_dir = train_dir + os.path.sep + cls_names[i]\n",
    "        if not os.path.exists(train_sub_dir):\n",
    "            os.makedirs(train_sub_dir)\n",
    "        train_sub_dirs.append(train_sub_dir)\n",
    "        val_sub_dir = val_dir + os.path.sep + cls_names[i]\n",
    "        if not os.path.exists(val_sub_dir):\n",
    "            os.makedirs(val_sub_dir)\n",
    "        val_sub_dirs.append(val_sub_dir)\n",
    "    return train_sub_dirs, val_sub_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvt2pngs(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: dir not exists.')\n",
    "        return\n",
    "    files = os.listdir(dir)\n",
    "    for f in files:\n",
    "        name = os.path.join(dir, f)\n",
    "        if os.path.isdir(name):\n",
    "            cvt2pngs(name)\n",
    "        else:\n",
    "            if os.path.split(f)[1] != '.png':\n",
    "                img = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
    "                if img.empty():\n",
    "                    continue\n",
    "                cv2.imshow('img', img)\n",
    "                cv2.waitKey(0)\n",
    "                f_cvt = os.path.splitext(f)[0] + '.png'\n",
    "                cv2.imwrite(f_cvt, img)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvt2png(dir):\n",
    "    '''\n",
    "    批量转换成png\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: dir not exists.')\n",
    "        return\n",
    "\n",
    "    g = os.walk(dir)\n",
    "    for path, dir_names, file_list in tqdm(g):\n",
    "        # print(dir_names)\n",
    "        for file_name in tqdm(file_list):\n",
    "            f = os.path.join(path, file_name)\n",
    "            # print(f)\n",
    "            if os.path.split(f)[1] != '.png':\n",
    "                img = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
    "                # cv2.imshow('img', img)\n",
    "                # cv2.waitKey(0)\n",
    "                f_cvt = os.path.splitext(f)[0] + '.png'\n",
    "                cv2.imwrite(f_cvt, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dirs(dir, limit=4):\n",
    "    '''\n",
    "    过滤数据集，统计人脸不少于limit个的目录\n",
    "    '''\n",
    "    dirs = []\n",
    "    items = os.listdir(dir)\n",
    "    for item in tqdm(items):\n",
    "        sub_dir_path = os.path.join(dir, item)\n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            if len(os.listdir(sub_dir_path)) >= limit:\n",
    "                # print('-- no less than %d: ' %limit, sub_dir_path)\n",
    "                dirs.append(sub_dir_path)\n",
    "    print('total %d dir meet requirements.' % len(dirs))\n",
    "    return dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_move(dir, limit):\n",
    "    '''\n",
    "    filter for \n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    sub_dirs = filter_dirs(dir, limit)\n",
    "    cls_names = [os.path.split(sub_dir)[1] for sub_dir in sub_dirs]\n",
    "    # print(cls_names)\n",
    "    # print(dirs)\n",
    "\n",
    "    # 上一级父目录\n",
    "    parent_dir = os.path.abspath(os.path.join(dir, \"..\"))\n",
    "    print(parent_dir)\n",
    "    train_dir = os.path.join(parent_dir + os.path.sep + 'train')\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    val_dir = os.path.join(parent_dir + os.path.sep + 'val')\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.makedirs(val_dir)\n",
    "\n",
    "    # 根据符合条件的目录数决定分类数量，创建子目录\n",
    "    train_sub_dirs, val_sub_dirs = create_sub_dirs_Name(\n",
    "        cls_names, train_dir, val_dir)\n",
    "    # print(train_sub_dirs)\n",
    "\n",
    "    # 生成训练，验证数据集\n",
    "    for i in range(len(cls_names)):\n",
    "        train_sub_items = [os.path.join(sub_dirs[i], x)\n",
    "                           for x in os.listdir(sub_dirs[i])[:14]]\n",
    "        val_sub_items = [os.path.join(sub_dirs[i], x)\n",
    "                         for x in os.listdir(sub_dirs[i])[14:20]]\n",
    "\n",
    "        # 批量拷贝\n",
    "        for train_item in train_sub_items:\n",
    "            shutil.copy(train_item, train_sub_dirs[i])\n",
    "        for val_item in val_sub_items:\n",
    "            shutil.copy(val_item, val_sub_dirs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_triplet(dir, limit=20, person_per_batch=5, img_per_person=10):\n",
    "    '''\n",
    "    prepare dataset in the triplet format\n",
    "    '''\n",
    "    np.random.seed(100)  # 设置固定的随机数种子,便于验证\n",
    "    # 随机选择一个反例id\n",
    "\n",
    "    def get_negative_id(sub_dirs, i):\n",
    "        candidates = [j for j in range(len(sub_dirs)) if j != i]\n",
    "        negative_id = np.random.choice(candidates, size=1)[0]\n",
    "        return negative_id\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    sub_dirs = filter_dirs(dir, limit)  # 筛选不少于limit张的dir\n",
    "    remain = int(len(sub_dirs) % person_per_batch)\n",
    "    sub_dirs = sub_dirs[:-remain]  # batch数据对齐\n",
    "    print('total: ', len(sub_dirs))\n",
    "\n",
    "    parent_dir = os.path.abspath(os.path.join(dir, \"..\"))\n",
    "    triplet_dir = os.path.join(parent_dir + os.path.sep + 'triplet_data')\n",
    "    if not os.path.exists(triplet_dir):\n",
    "        os.makedirs(triplet_dir)\n",
    "\n",
    "    # 遍历符合要求sub_dir\n",
    "    for i, sub_dir in enumerate(sub_dirs):\n",
    "        # 处理每个batch\n",
    "        if i % person_per_batch == 0:\n",
    "            batch_id = int(i / person_per_batch)\n",
    "            batch_sub_ids = [i + person_per_batch -\n",
    "                             k for k in range(person_per_batch, 0, -1)]\n",
    "            print('batch_id: ', batch_id, ', batch_sub_ids: ', batch_sub_ids)\n",
    "            for sub_id in batch_sub_ids:\n",
    "                # anchors\n",
    "                anchors = [os.path.join(sub_dirs[sub_id], x)\n",
    "                           for x in os.listdir(sub_dirs[sub_id])[:img_per_person]]\n",
    "                for item in anchors:\n",
    "                    shutil.copy(item, triplet_dir)\n",
    "\n",
    "                # positive\n",
    "                positives = [os.path.join(sub_dirs[sub_id], x)\n",
    "                             for x in os.listdir(sub_dirs[sub_id])[img_per_person:limit]]\n",
    "                for item in positives:\n",
    "                    shutil.copy(item, triplet_dir)\n",
    "\n",
    "                # negative\n",
    "                negative_id = get_negative_id(sub_dirs, sub_id)\n",
    "                negatives = [os.path.join(sub_dirs[negative_id], x)\n",
    "                             for x in os.listdir(sub_dirs[negative_id])[:img_per_person]]\n",
    "                for item in negatives:\n",
    "                    shutil.copy(item, triplet_dir)\n",
    "                pass\n",
    "# def select_triplet(dir, limit=20):\n",
    "#     '''\n",
    "#     从数据及随机选择一个三元组\n",
    "#     '''\n",
    "#     np.random.seed(100)  # 设置固定的随机数种子,便于验证\n",
    "#     # 随机选择一个反例id\n",
    "\n",
    "#     def get_negative_id(length, i):\n",
    "#         candidates = [j for j in range(length) if j != i]\n",
    "#         negative_id = np.random.choice(candidates, size=1)[0]\n",
    "#         return negative_id\n",
    "\n",
    "#     if not os.path.exists(dir):\n",
    "#         print('Error: invalid dir.')\n",
    "#         return\n",
    "\n",
    "#     sub_dirs = filter_dirs(dir, limit)  # 筛选不少于limit张的dir\n",
    "\n",
    "#     # 随机选择一个anchor, positive, negative实例三元组\n",
    "#     anchor_cls = np.random.choice(len(sub_dirs))\n",
    "#     negative_cls = get_negative_id(len(sub_dirs), anchor_cls)\n",
    "#     print('anchor_cls: %d, negative_cls: %d' %(anchor_cls, negative_cls))\n",
    "#     anchor_id = np.random.choice(range(len(os.listdir(sub_dirs[anchor_cls]))))\n",
    "#     positive_id = get_negative_id(len(os.listdir(sub_dirs[anchor_cls])), anchor_id)\n",
    "#     # print('anchor_id: %d, positive id: %d' %(anchor_id, positive_id))\n",
    "#     negative_id = np.random.choice(range(len(os.listdir(sub_dirs[negative_cls]))))\n",
    "#     print('negative_id: ', negative_id)\n",
    "\n",
    "#     anchor_path = os.path.join(sub_dirs[anchor_cls], os.listdir(sub_dirs[anchor_cls])[anchor_id])\n",
    "#     print('anchor_path: ', anchor_path)\n",
    "#     positive_path = os.path.join(sub_dirs[anchor_cls], os.listdir(sub_dirs[anchor_cls])[positive_id])\n",
    "#     print('positive_path: ', positive_path)\n",
    "#     negative_path = os.path.join(sub_dirs[negative_cls], os.listdir(sub_dirs[negative_cls])[negative_id])\n",
    "#     print('negative_path: ', negative_path)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_triplet(dir, limit=20):\n",
    "    '''\n",
    "    创建三元组数据集\n",
    "    '''\n",
    "    np.random.seed(100)  # 设置固定的随机数种子,便于验证\n",
    "    # 随机选择一个反例id\n",
    "\n",
    "    def get_negative_id(length, i):\n",
    "        candidates = [j for j in range(length) if j != i]\n",
    "        negative_id = np.random.choice(candidates, size=1)[0]\n",
    "        return negative_id\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    parent_dir = os.path.abspath(os.path.join(dir, \"..\"))\n",
    "    triplet_dir = os.path.join(parent_dir + os.path.sep + 'triplet_data')\n",
    "    if not os.path.exists(triplet_dir):\n",
    "        os.makedirs(triplet_dir)\n",
    "\n",
    "    sub_dirs = filter_dirs(dir, limit)  # 筛选不少于limit张的dir\n",
    "    for sub_dir in tqdm(sub_dirs):\n",
    "        for item in os.listdir(sub_dir)[:limit]:\n",
    "            shutil.copy(os.path.join(sub_dir, item), triplet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选择一个反例id\n",
    "def get_negative_id(length, i):\n",
    "    candidates = [j for j in range(length) if j != i]\n",
    "    negative_id = np.random.choice(candidates, size=1)[0]\n",
    "    return negative_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_triplet(dir, num_classes, limit=20, is_car=False):\n",
    "    '''\n",
    "    从triplet_dir中随机选择一个三元组\n",
    "    '''\n",
    "    # np.random.seed(100)  # 设置固定的随机数种子,便于验证\n",
    "\n",
    "    # 获取anchor, positive, negative 图片ID\n",
    "    anchor_cls = np.random.choice(num_classes)\n",
    "    anchor_id = anchor_cls*limit + np.random.choice(limit)\n",
    "    positive_id = anchor_cls*limit + \\\n",
    "        get_negative_id(limit, anchor_id - anchor_cls*limit)\n",
    "    negative_cls = get_negative_id(num_classes, anchor_cls)  # 随机选择一个反例类型\n",
    "    negative_id = negative_cls*limit + np.random.choice(limit)  # 随机选择一个反例ID\n",
    "\n",
    "    # 获取anchor, positive, negative 图片地址\n",
    "    file_names = os.listdir(dir)\n",
    "    if is_car:  # 汽车图片被重命名过,因此需要按照(数值大小)从小到大排序\n",
    "        file_names.sort(key=lambda x: int(re.match(r'(\\d+)\\.jpg', x).group(1)))\n",
    "\n",
    "    anchor_path = os.path.join(dir, file_names[anchor_id])\n",
    "    positive_path = os.path.join(dir, file_names[positive_id])\n",
    "    negative_path = os.path.join(dir, file_names[negative_id])\n",
    "\n",
    "    # 返回6元组\n",
    "    return anchor_path, positive_path, negative_path, anchor_cls, anchor_cls, negative_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_car_triplet(dir, labels, label2img):\n",
    "    '''\n",
    "    从triplet_dir中随机选择一个三元组: 图像并没有重命名\n",
    "    '''\n",
    "    anchor_cls = np.random.choice(len(labels))\n",
    "    anchor_id = np.random.choice(len(label2img[labels[anchor_cls]]))\n",
    "    positive_id = get_negative_id(\n",
    "        len(label2img[labels[anchor_cls]]), anchor_id)\n",
    "    negative_cls = get_negative_id(len(labels), anchor_cls)\n",
    "    negative_id = np.random.choice(len(label2img[labels[negative_cls]]))\n",
    "\n",
    "    # anchor, positive, negative文件路径\n",
    "    anchor_path = os.path.join(dir, label2img[labels[anchor_cls]][anchor_id])\n",
    "    positive_path = os.path.join(\n",
    "        dir, label2img[labels[anchor_cls]][positive_id])\n",
    "    negative_path = os.path.join(\n",
    "        dir, label2img[labels[negative_cls]][negative_id])\n",
    "\n",
    "    return anchor_path, positive_path, negative_path, \\\n",
    "        labels[anchor_cls], labels[anchor_cls], labels[negative_cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_triplets(dir, limit=20):\n",
    "    '''\n",
    "    计算所有的triplet\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    sub_dirs = filter_dirs(dir, limit)  # 筛选不少于limit张的dir\n",
    "\n",
    "    triplets = []\n",
    "    for anchor_dir in tqdm(sub_dirs):\n",
    "        anchor_paths = [os.path.join(anchor_dir, x)\n",
    "                        for x in os.listdir(anchor_dir)]\n",
    "        for anchor_path in anchor_paths:\n",
    "            for positive_path in [path for path in anchor_paths if path != anchor_path]:\n",
    "                # print('anchor_path: %s, positive_path: %s' %(anchor_path, positive_path))\n",
    "                for negative_dir in [the_dir for the_dir in sub_dirs if the_dir != anchor_dir]:\n",
    "                    negative_paths = [os.path.join(\n",
    "                        negative_dir, x) for x in os.listdir(negative_dir)]\n",
    "                    for negative_path in negative_paths:\n",
    "                        anchor_label = sub_dirs.index(anchor_dir)\n",
    "                        negative_label = sub_dirs.index(negative_dir)\n",
    "                        triplet = anchor_path, positive_path, negative_path, \\\n",
    "                            anchor_label, anchor_label, negative_label\n",
    "                        print(triplet)\n",
    "                        triplets.append(triplet)\n",
    "    # print(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 为测试从dataset输入数据集的方法, 生成所有数据在一个目录的文件夹\n",
    "def prepare_for_dataset(dir, limit=20):\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    parent_dir = os.path.abspath(os.path.join(dir, \"..\"))\n",
    "    my_dataset = os.path.join(parent_dir + os.path.sep + 'my_dataset')\n",
    "    if not os.path.exists(my_dataset):\n",
    "        os.makedirs(my_dataset)\n",
    "\n",
    "    sub_dirs = filter_dirs(dir, limit)  # 筛选不少于limit张的dir\n",
    "    for sub_dir in tqdm(sub_dirs):\n",
    "        for f in os.listdir(sub_dir)[:limit]:\n",
    "            shutil.copy(os.path.join(sub_dir, f), my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------创建验证数据集\n",
    "def create_validate_set(dir, limit=20, num=100):\n",
    "    '''\n",
    "    @param dirs: valid dirs with enough examplars\n",
    "    基本要求: 数据与label对应起来\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    dirs = filter_dirs(dir, limit)  # 筛选不少于limit张的dir\n",
    "    dirs.sort()\n",
    "    if len(dirs) == 0:\n",
    "        print('Error: invalid dirs.')\n",
    "        return\n",
    "    print('mapping:\\n')\n",
    "    for label, name in enumerate(dirs):\n",
    "        print('[%d, %s]' % (label, name))\n",
    "\n",
    "    # 创建validation set目录\n",
    "    parant_dir = os.path.abspath(os.path.join(dir, '..'))\n",
    "    validset_path = os.path.join(parant_dir + os.path.sep + 'validate_set')\n",
    "    print(validset_path)\n",
    "    if os.path.exists(validset_path):\n",
    "        shutil.rmtree(validset_path)  # 如果已经存在, 清空目录\n",
    "        os.makedirs(validset_path)\n",
    "    else:\n",
    "        os.makedirs(validset_path)\n",
    "\n",
    "    data_label = {}\n",
    "    for i in tqdm(range(num)):\n",
    "        dir_id = np.random.choice(len(dirs))\n",
    "\n",
    "        # 生成validation数据集\n",
    "        items = os.listdir(dirs[dir_id])\n",
    "        item_id = np.random.choice(len(items))\n",
    "        item_path = os.path.join(dirs[dir_id], items[item_id])\n",
    "        dst_path = os.path.join(validset_path, os.path.split(item_path)[1])\n",
    "        while os.path.exists(dst_path):  # 如果已经存在于validset就重新选择一个\n",
    "            item_id = np.random.choice(len(items))\n",
    "            item_path = os.path.join(dirs[dir_id], items[item_id])\n",
    "            dst_path = os.path.join(validset_path, os.path.split(item_path)[1])\n",
    "        data_label[items[item_id]] = dir_id\n",
    "        shutil.copy(item_path, validset_path)\n",
    "\n",
    "    # 保存label文件\n",
    "    labels = [v for k, v in data_label.items()]\n",
    "    labels.sort()\n",
    "    print(labels)\n",
    "    label_path = os.path.join(validset_path, 'labels.txt')\n",
    "    if os.path.exists(label_path):\n",
    "        os.remove(label_path)\n",
    "    with open(label_path, 'w') as f:\n",
    "        for label in labels:\n",
    "            f.write(str(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validate_set(dir, num_per_cls=10, limit=20):\n",
    "    '''\n",
    "    创建验证数据集\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    dirs = filter_dirs(dir, limit)  # 筛选不少于limit张的dir\n",
    "    dirs.sort()\n",
    "    if len(dirs) == 0:\n",
    "        print('Error: invalid dirs.')\n",
    "        return\n",
    "    # print('mapping:\\n')\n",
    "    # for label, name in enumerate(dirs):\n",
    "    #     print('[%d, %s]' %(label, name))\n",
    "\n",
    "    # 创建validation set目录\n",
    "    parant_dir = os.path.abspath(os.path.join(dir, '..'))\n",
    "    validset_path = os.path.join(parant_dir + os.path.sep + 'validate_set')\n",
    "    print(validset_path)\n",
    "    if os.path.exists(validset_path):\n",
    "        shutil.rmtree(validset_path)  # 如果已经存在, 清空目录\n",
    "        os.makedirs(validset_path)\n",
    "    else:\n",
    "        os.makedirs(validset_path)\n",
    "\n",
    "    # 创建validate_set数据集\n",
    "    for i_, dir_ in tqdm(enumerate(dirs)):\n",
    "        for i in range(num_per_cls):  # 每个类别数量相同\n",
    "            items = os.listdir(dir_)\n",
    "            item_id = np.random.choice(len(items))  # 随机选择一个该类别的样本\n",
    "            item_path = os.path.join(dir_, items[item_id])\n",
    "            dst_path = os.path.join(validset_path, os.path.split(item_path)[1])\n",
    "            while os.path.exists(dst_path):  # 如果已经存在于validset就重新选择一个\n",
    "                item_id = np.random.choice(len(items))\n",
    "                item_path = os.path.join(dir_, items[item_id])\n",
    "                dst_path = os.path.join(\n",
    "                    validset_path, os.path.split(item_path)[1])\n",
    "            shutil.copy(item_path, validset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取.mat标签文件\n",
    "import scipy.io as scio\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_class_names(path):\n",
    "    '''\n",
    "    @param path: meta mat path\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        print('Error: invalid path.')\n",
    "        return\n",
    "\n",
    "    data = scio.loadmat(path)\n",
    "    # print(data.keys(), '\\n')\n",
    "    # print('__header__: ', data['__header__'])\n",
    "    # print('__version__: ', data['__version__'])\n",
    "    # print('__global__: ', data['__globals__'])\n",
    "\n",
    "    class_names = [x[0] for x in data['class_names'][0]]\n",
    "    # print(class_names)\n",
    "    return class_names  # 依据car的label(int)索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imgname_label(path):\n",
    "    '''\n",
    "    @param path: mat文件path\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        print('Error: invalid path.')\n",
    "        return\n",
    "\n",
    "    data = scio.loadmat(path)\n",
    "    annots = data['annotations']\n",
    "    annots = annots[0]\n",
    "\n",
    "    img_names = [str(x[-1][0]) for x in annots]\n",
    "    img_labels = [x[-2][0][0] for x in annots]\n",
    "    # print(img_labels)\n",
    "\n",
    "    id_imgs = {}\n",
    "    img_name2labels = {}\n",
    "    label_img_names = defaultdict(list)\n",
    "    for id, (name, label) in enumerate(zip(img_names, img_labels)):\n",
    "        # print('%d, %s, %d' %(id, name, label))\n",
    "        id_imgs[id] = (name, label)  # 数据集中第id张图的信息\n",
    "\n",
    "        img_name2labels[name] = label  # 记录每个文件名对应的label\n",
    "        label_img_names[label].append(name)  # 记录每个label对应所有文件名\n",
    "        # print('id: %d, img_name: %s, label: %d' %(id, id_imgs[id][0], id_imgs[id][1]))\n",
    "    return id_imgs, img_labels, img_name2labels, label_img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cars(meta_path, annot_path, orig_train_dir, limit=40):\n",
    "    '''\n",
    "    根据元信息和annotations读取labels\n",
    "    '''\n",
    "    class_names = read_class_names(meta_path)\n",
    "    print('total %d kind of cars.' % len(class_names))\n",
    "\n",
    "    id_imgs, labels, img_names2label, label2img_names = read_imgname_label(\n",
    "        annot_path)\n",
    "    print(len(img_names2label))  # 所有训练数据集, 名字->label映射\n",
    "\n",
    "    # for (k, v) in id_imgs.items():\n",
    "    #     print('file_%d, img_name: %s, label: %d, class_name: %s' %\n",
    "    #           (k, v[0], v[1], class_names[v[1] - 1])) # 从class label转换成label索引,需要-1\n",
    "\n",
    "    # 统计训练数据及中196种车型各自的图片数量\n",
    "    freqs = defaultdict(int)\n",
    "    for label in labels:\n",
    "        freqs[label] += 1\n",
    "    sorted_freqs = sorted(freqs.items(), key=lambda label: label[1])\n",
    "    # print(sorted_freqs)  # 按频数从小到大排序\n",
    "\n",
    "    # 创建训练目录\n",
    "    train_dir = os.path.join(os.getcwd(), 'car_train_data')\n",
    "    if os.path.exists(train_dir):\n",
    "        shutil.rmtree(train_dir)  # 如果已经存在, 清空目录,重新创建目录\n",
    "        os.makedirs(train_dir)\n",
    "    else:\n",
    "        os.makedirs(train_dir)\n",
    "\n",
    "    labels = sorted(set(labels))\n",
    "    # print(labels) # 1~196\n",
    "    # 获取label到类别名称的映射\n",
    "    label2_class_names = defaultdict(str)\n",
    "    for label in labels:\n",
    "        # label转换成label的索引, 需要-1\n",
    "        label2_class_names[label] = class_names[label - 1]\n",
    "        # print(label2_class_names[label])\n",
    "\n",
    "    # 将数量>limit的车copy到一个训练目录\n",
    "    qualified_labels = []\n",
    "    # new2old_lables = defaultdict(int)\n",
    "    new_lable2classname = defaultdict(str)\n",
    "    i = 0\n",
    "    for label in tqdm(labels):\n",
    "        if freqs[label] >= limit:  # 满足数量要求的图片\n",
    "            qualified_labels.append(label)\n",
    "            for img_file in label2img_names[label]:\n",
    "                shutil.copy(os.path.join(orig_train_dir, img_file), train_dir)\n",
    "\n",
    "            # -----------------------------------------\n",
    "            # # 每个类别, 只拷贝limit个, 拷贝并重命名\n",
    "            # item_ids = np.random.choice(len(label2img_names[label]), size=limit)\n",
    "            # for j, item_id in enumerate(item_ids):\n",
    "            #     start_id = i*limit\n",
    "            #     dst_path = os.path.join(train_dir, str(start_id + j) + '.jpg')\n",
    "            #     src_path = os.path.join(\n",
    "            #         orig_train_dir, label2img_names[label][item_id])\n",
    "            #     shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "            #     # 判断通道数, 将1通道数据转换成RGB标准3通道\n",
    "            #     img = cv2.imread(dst_path, cv2.IMREAD_UNCHANGED)\n",
    "            #     num_channels = len(cv2.split(img))\n",
    "            #     if num_channels == 1:\n",
    "            #         img_cvt = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "            #         cv2.imwrite(dst_path, img_cvt)\n",
    "            #         print('converted channel number: %d,' % len(cv2.split(img_cvt)), end='')\n",
    "            #         print(' converted from 1 channel to 3 channels.')\n",
    "            # # new2old_lables[i] = label\n",
    "            # new_lable2classname[i] = label2_class_names[label]  # 车类别的实际名称\n",
    "            # # print('%d mapping to %d: %s' % (i, label, new_lable2classname[i]))\n",
    "            # -----------------------------------------\n",
    "            i += 1\n",
    "\n",
    "    # 预处理每一个类别的数据: 将只有1通道的图像转换成RGB标准3通道\n",
    "    files = os.listdir(train_dir)\n",
    "    for f in files:\n",
    "        f_path = os.path.join(train_dir, f)\n",
    "        img = cv2.imread(f_path, cv2.IMREAD_UNCHANGED)\n",
    "        if len(cv2.split(img)) == 1:\n",
    "            print('%s is 1 channel image, ' % f, end='')\n",
    "            img_cvt = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "            cv2.imwrite(f_path, img_cvt)\n",
    "            print('%s is converted to %d channels.' %\n",
    "                  (f, len(cv2.split(img_cvt))))\n",
    "    print('total %d kind of cars meet requirements.' % i)\n",
    "    # print('qualified labels:\\n', qualified_labels)\n",
    "\n",
    "    # 验证正确性\n",
    "    # for label in qualified_labels:\n",
    "    #     print('\\nqualifieed class_name: {}'.format(label2_class_names[label]))\n",
    "    #     print([label2img_names[label]])\n",
    "\n",
    "    # 计算每个qualified label到改label对应所有文件名的映射\n",
    "    qual_label2img_names = defaultdict(list)\n",
    "    for label in qualified_labels:\n",
    "        qual_label2img_names[label] = label2img_names[label]\n",
    "\n",
    "    # 将类别标签, 类别名称等信息序列化到硬盘\n",
    "    label_dict_file = os.path.join(os.getcwd(), 'img_names2labels.pkl')\n",
    "    qualified_label_file = os.path.join(os.getcwd(), 'quali_labels.pkl')\n",
    "    qual_label2img_names_file = os.path.join(\n",
    "        os.getcwd(), 'label2img_names.pkl')\n",
    "    class_names_file = os.path.join(os.getcwd(), 'class_names.pkl')\n",
    "    new_lable2classname_file = os.path.join(\n",
    "        os.getcwd(), 'new_lable2classname.pkl')\n",
    "\n",
    "    pickle.dump(qualified_labels, open(\n",
    "        qualified_label_file, 'wb'))  # 记录有效的label\n",
    "    pickle.dump(img_names2label, open(\n",
    "        label_dict_file, 'wb'))  # 记录每个文件名对应的label\n",
    "    pickle.dump(qual_label2img_names, open(\n",
    "        qual_label2img_names_file, 'wb'))  # 记录每个有效label对应的所有文件名\n",
    "    pickle.dump(class_names, open(class_names_file, 'wb'))\n",
    "    pickle.dump(new_lable2classname, open(new_lable2classname_file, 'wb'))\n",
    "    return qualified_labels, img_names2label\n",
    "\n",
    "\n",
    "# def process_car(meta_path, annot_path, orig_train_dir, limit=40):\n",
    "#     class_names = read_class_names(meta_path)\n",
    "#     print('total %d kind of cars.' % len(class_names))\n",
    "\n",
    "#     id_imgs, labels, img_names2label, label2img_names = read_imgname_label(\n",
    "#         annot_path)\n",
    "\n",
    "#     #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_car_triplet():\n",
    "    '''\n",
    "    测试car triplet选择是否正确\n",
    "    '''\n",
    "    quali_labels = pickle.load(open('quali_labels.pkl', 'rb'))\n",
    "    img_name2labels = pickle.load(open('img_names2labels.pkl', 'rb'))\n",
    "    label2img_names = pickle.load(open('label2img_names.pkl', 'rb'))\n",
    "    class_names = pickle.load(open('class_names.pkl', 'rb'))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(3):\n",
    "        a, b, c, d, e, f = select_car_triplet('car_train_data',\n",
    "                                              quali_labels,\n",
    "                                              label2img_names)\n",
    "        print('{}, {}, {}'.format(\n",
    "            class_names[d-1], class_names[e-1], class_names[f-1]))\n",
    "\n",
    "        anchor = Image.open(a)\n",
    "        positive = Image.open(b)\n",
    "        negative = Image.open(c)\n",
    "\n",
    "        anchor = anchor.resize((400, 400))\n",
    "        positive = positive.resize((400, 400))\n",
    "        negative = negative.resize((400, 400))\n",
    "\n",
    "        ax_0 = plt.subplot(131)\n",
    "        ax_1 = plt.subplot(132)\n",
    "        ax_2 = plt.subplot(133)\n",
    "\n",
    "        ax_0.imshow(anchor)\n",
    "        ax_1.imshow(positive)\n",
    "        ax_2.imshow(negative)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_imgs(dir, width, height):\n",
    "    '''\n",
    "    将目录下所有图片缩放到指定大小\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    f_names = os.listdir(dir)\n",
    "    # print(f_names)\n",
    "\n",
    "    for f in tqdm(f_names):\n",
    "        f_name = os.path.join(dir, f)\n",
    "        if os.path.isfile(f_name):\n",
    "            if os.path.splitext(f_name)[1] == '.jpg':\n",
    "                img = cv2.imread(f_name, cv2.IMREAD_UNCHANGED)\n",
    "                if img.shape[0] != height and img.shape[1] != width:\n",
    "                    img = cv2.resize(img, (height, width),\n",
    "                                     interpolation=cv2.INTER_CUBIC)\n",
    "                    cv2.imwrite(f_name, img)  # 写入所犯之后的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_channel(dir):\n",
    "    '''\n",
    "    判断一个目下的图像是灰度图还是彩色图\n",
    "    '''\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "\n",
    "    for f in os.listdir(dir):\n",
    "        f_path = os.path.join(dir, f)\n",
    "        if os.path.isfile(f_path):\n",
    "            img = cv2.imread(f_path, cv2.IMREAD_UNCHANGED)\n",
    "            channels = cv2.split(img)\n",
    "            if len(channels) == 1:\n",
    "                print('%s is 1 channel image,' % (f_path), end='')\n",
    "                img_cvt = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                cns = cv2.split(img_cvt)\n",
    "                cv2.imwrite(f_path, img_cvt)\n",
    "                print(' converted {} channels.'.format(len(cns)))\n",
    "                # cv2.imshow('3 channel', img)\n",
    "                # cv2.waitKey()\n",
    "            # elif len(channels) == 3:\n",
    "            #     print('%s is 3 channel image.' %(f_path))\n",
    "            elif len(channels) == 4:\n",
    "                print('%s is 4 channel image.' % (f_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从人脸训练数据集中随机生成一个测试数据集\n",
    "def get_test_from_train(dir, num_classes=62, num_per_cls=20):\n",
    "    if not os.path.exists(dir):\n",
    "        print('Error: invalid dir.')\n",
    "        return\n",
    "    parent_dir = os.path.realpath(os.path.join(dir, '..'))\n",
    "    test_set = os.path.join(parent_dir + os.path.sep + 'test_set')\n",
    "    if os.path.exists(test_set):\n",
    "        shutil.rmtree(test_set)\n",
    "        os.makedirs(test_set)\n",
    "    else:\n",
    "        os.makedirs(test_set)\n",
    "\n",
    "    files = os.listdir(dir)\n",
    "    for i in tqdm(range(num_classes)):\n",
    "        ids = np.random.choice(num_per_cls,\n",
    "                               int(num_per_cls * 0.5),\n",
    "                               replace=False)  # 不放回抽取\n",
    "        start_id = i * num_per_cls\n",
    "        for id in ids:\n",
    "            id += start_id\n",
    "            src_path = os.path.join(dir, files[id])\n",
    "            shutil.copy(src_path, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off-line all hard triplets mining\n",
    "def get_all_hard_triplets(dir, model_path,\n",
    "                          num_classes=62, num_per_cls=20):\n",
    "    '''\n",
    "    在train_set中offline hard sample mining的方式选择所有triplets\n",
    "    '''\n",
    "    # 加载特征提取CNN模型\n",
    "    model = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "\n",
    "    # 将全连接层分类器转换成为恒等映射, 用来提取深度特征\n",
    "    del model._classifier\n",
    "    model._classifier = lambda x: x\n",
    "    # model.cuda()  # 模型放进GPU\n",
    "    model.cpu()  # 模型放进CPU\n",
    "    model.eval()  # 求值模式\n",
    "\n",
    "    # 训练数据集所有文件名\n",
    "    files = os.listdir(dir)\n",
    "\n",
    "    # 图像处理方式\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=np.array([0.485, 0.456, 0.406]),\n",
    "            std=np.array([0.229, 0.224, 0.225])),\n",
    "    ])\n",
    "\n",
    "    triplets = []  # 存放所有被选择的triplet\n",
    "\n",
    "    # 遍历每一个类别\n",
    "    for anchor_cls in tqdm(range(num_classes)):\n",
    "        positive_cls = anchor_cls\n",
    "\n",
    "        # 随机选择一个正样本作为anchor:不应该随机选择,应该遍历所有id\n",
    "        for id in range(num_per_cls):\n",
    "            anchor_id = anchor_cls * num_per_cls + id # np.random.choice(num_per_cls)\n",
    "            anchor_path = os.path.join(dir, files[anchor_id])\n",
    "            # anchor_img = Variable(transform(Image.open(anchor_path)).unsqueeze(0),\n",
    "            #                       volatile=True).cuda()  # 数据放入GPU\n",
    "            with torch.no_grad():\n",
    "                anchor_img = Variable(transform(Image.open(anchor_path)).unsqueeze(0)\n",
    "                                    )  # 数据放入CPU\n",
    "            anchor_feats = model.forward(anchor_img)\n",
    "            anchor_feats = torch.div(\n",
    "                anchor_feats, torch.norm(anchor_feats, 2))  # 特征向量L2规范化\n",
    "            # print('anchor_feats:\\n', anchor_feats)\n",
    "\n",
    "            # 遍历每一个负样本选择最hard的一个\n",
    "            negative_cls = -1\n",
    "            negative_id = -1\n",
    "            negative_path = ''\n",
    "            dist = float('inf')  # 无穷大\n",
    "            for nega_cls in range(num_classes):# 遍历每一个负样本类别\n",
    "                if nega_cls == anchor_cls:\n",
    "                    continue\n",
    "                nega_start = nega_cls * num_per_cls\n",
    "                # 遍历所有负样本的类别的所有负样本文件\n",
    "                for nega_id in range(nega_start, nega_start + num_per_cls):\n",
    "                    nega_path = os.path.join(dir, files[nega_id])\n",
    "                    # nega_img = Variable(transform(Image.open(nega_path)).unsqueeze(0),\n",
    "                    #                     volatile=True).cuda()  # 数据放入GPU\n",
    "                    with torch.no_grad():\n",
    "                        nega_img = Variable(transform(Image.open(nega_path)).unsqueeze(0)\n",
    "                                            )  # 数据放入CPU\n",
    "                    nega_feats = model.forward(nega_img)\n",
    "\n",
    "                    # 特征L2张量规范化\n",
    "                    nega_feats = torch.div(nega_feats, torch.norm(nega_feats, 2))\n",
    "\n",
    "                    # 计算L2规范化后的特征向量L2距离\n",
    "                    nega_anchor_dist = F.pairwise_distance(\n",
    "                        nega_feats, anchor_feats, p=2).cpu().data.numpy()[0]\n",
    "                    if nega_anchor_dist < dist:\n",
    "                        dist = nega_anchor_dist\n",
    "                        negative_cls = nega_cls\n",
    "                        negative_id = nega_id\n",
    "\n",
    "            # 计算该ID的每一对anchor——positive正样本对\n",
    "            if negative_id != -1:\n",
    "                negative_path = os.path.join(dir, files[negative_id])\n",
    "                positive_ids = [positive_cls * num_per_cls +\n",
    "                                id for id in range(num_per_cls)]\n",
    "                positive_ids = [id for id in positive_ids if id != anchor_id]\n",
    "                for positive_id in positive_ids:\n",
    "                    positive_path = os.path.join(dir, files[positive_id])\n",
    "                    triplets.append((anchor_path, positive_path, negative_path,\n",
    "                                     anchor_cls, positive_cls, negative_cls))\n",
    "            else:\n",
    "                print('invalid negative sample.')\n",
    "    print(triplets)\n",
    "    print('total %d triplets.' % len(triplets))\n",
    "\n",
    "    # 应该把这个triplets存到硬盘\n",
    "    pickle.dump(triplets, open('triplets.pkl', 'wb'))\n",
    "    return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'cnn_finetune.contrib.torchvision.ResNetWrapper' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torchvision.models.resnet.BasicBlock' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AdaptiveAvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\jyli39\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\serialization.py:671: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "  2%|▏         | 1/62 [42:07<42:50:07, 2527.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6816/223166303.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# judge_channel('car_train_data')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# get_test_from_train('train_set')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mget_all_hard_triplets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_set'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'checkpoints/epoch_35.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n--Test done.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6816/2958601396.py\u001b[0m in \u001b[0;36mget_all_hard_triplets\u001b[1;34m(dir, model_path, num_classes, num_per_cls)\u001b[0m\n\u001b[0;32m     64\u001b[0m                         nega_img = Variable(transform(Image.open(nega_path)).unsqueeze(0)\n\u001b[0;32m     65\u001b[0m                                             )  # 数据放入CPU\n\u001b[1;32m---> 66\u001b[1;33m                     \u001b[0mnega_feats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnega_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                     \u001b[1;31m# 特征L2张量规范化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\cnn_finetune\\base.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\cnn_finetune\\base.py\u001b[0m in \u001b[0;36mfeatures\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0midentity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 443\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # cvt2png('./YaleFace')\n",
    "    # process_data('./YaleFace', 15, 11, 4)\n",
    "    # filter_dirs('lfw', 20)\n",
    "    # prepare_for_dataset('lfw', 20)\n",
    "    # filter_move('lfw', 20)\n",
    "    # format_for_triplet('lfw')\n",
    "    # select_triplet('lfw')\n",
    "    # move_triplet('lfw')\n",
    "    # select_triplet('triplet_data', 62)\n",
    "    # get_all_triplets('lfw')\n",
    "    # get_validate_set('lfw')\n",
    "    # read_imgname_label('car_devkit/cars_train_annos.mat')\n",
    "    # read_class_names('car_devkit/cars_meta.mat')\n",
    "    # process_cars('car_devkit/cars_meta.mat',\n",
    "    #              'car_devkit/cars_train_annos.mat',\n",
    "    #              'cars_train')\n",
    "    # resize_imgs('car_train_data', 250, 250)\n",
    "    # test_car_triplet()\n",
    "    # select_car_triplet('car_train_data', 196, limit=20)\n",
    "    # judge_channel('car_train_data')\n",
    "    # get_test_from_train('train_set')\n",
    "    get_all_hard_triplets('train_set', 'checkpoints/epoch_35.pth')\n",
    "    print('\\n--Test done.')\n",
    "\n",
    "\n",
    "# pytorch数据集的准备工作\n",
    "# http://www.bubuko.com/infodetail-2304938.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18e351b8f43a8051a413352beddec2a7234c5048dc672983c8d1a2c12b0911bb"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
